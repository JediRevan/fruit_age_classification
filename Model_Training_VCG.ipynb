{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 52/52 [00:39<00:00,  1.32it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 7.6204, Val Loss: 6.5982\n",
      "Validation loss improved, saving model to checkpoint.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 52/52 [00:43<00:00,  1.20it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 1.9586, Val Loss: 4.7411\n",
      "Validation loss improved, saving model to checkpoint.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 52/52 [00:41<00:00,  1.26it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 0.7577, Val Loss: 4.9256\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 52/52 [00:41<00:00,  1.27it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 0.4888, Val Loss: 5.4494\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 52/52 [00:38<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 0.4278, Val Loss: 5.3095\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 52/52 [00:38<00:00,  1.34it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 0.2817, Val Loss: 5.8095\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 52/52 [00:39<00:00,  1.31it/s]\n",
      "Validating: 100%|██████████| 7/7 [00:02<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 0.2350, Val Loss: 5.2798\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Test Loss: 8.2214\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spectral.io.envi import open as envi_open\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "# --- Constants ---\n",
    "DATA_DIR = \"./VIS\"  # Update with your dataset path\n",
    "IMG_SIZE = 224  # Resize images to ResNet-compatible size\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def read_hyperspectral_image(hdr_file):\n",
    "    \"\"\"Load and preprocess hyperspectral image from .hdr and its associated .bin file.\"\"\"\n",
    "    bin_file = hdr_file.replace(\".hdr\", \".bin\")\n",
    "    try:\n",
    "        img = envi_open(hdr_file, image=bin_file).load()\n",
    "        img = np.mean(img, axis=2)\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = transforms.Resize((IMG_SIZE, IMG_SIZE))(img)\n",
    "        img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading image: {hdr_file}\") from e\n",
    "\n",
    "def get_image_metadata(file_name):\n",
    "    parts = file_name.split(\"_\")\n",
    "    day = int(parts[2])  # Extract day (e.g., 'day_10' -> 10)\n",
    "    mango_id = parts[4].split(\".\")[0]\n",
    "    return mango_id, day\n",
    "\n",
    "def create_pairs(data):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for mango_id, group in data.groupby(\"mango_id\"):\n",
    "        group = group.sort_values(\"day\")\n",
    "        files = group[\"file\"].tolist()\n",
    "        days = group[\"day\"].tolist()\n",
    "        for i in range(len(files) - 1):\n",
    "            for j in range(i + 1, len(files)):\n",
    "                if days[i] != days[j]:\n",
    "                    pairs.append((files[i], files[j]))\n",
    "                    labels.append(abs(days[j] - days[i]))\n",
    "    return pairs, labels\n",
    "\n",
    "def validate_pairs_and_labels(pairs, labels):\n",
    "    valid_pairs = []\n",
    "    valid_labels = []\n",
    "    for i, (file1, file2) in enumerate(pairs):\n",
    "        hdr1, hdr2 = file1.replace(\".bin\", \".hdr\"), file2.replace(\".bin\", \".hdr\")\n",
    "        bin1, bin2 = hdr1.replace(\".hdr\", \".bin\"), hdr2.replace(\".hdr\", \".bin\")\n",
    "        if os.path.exists(hdr1) and os.path.exists(hdr2) and os.path.exists(bin1) and os.path.exists(bin2):\n",
    "            valid_pairs.append((file1, file2))\n",
    "            valid_labels.append(labels[i])\n",
    "    return valid_pairs, valid_labels\n",
    "\n",
    "# --- Data Preparation ---\n",
    "data = []\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(\".bin\"):\n",
    "            try:\n",
    "                mango_id, day = get_image_metadata(file)\n",
    "                hdr_file = os.path.join(root, file.replace(\".bin\", \".hdr\"))\n",
    "                if os.path.exists(hdr_file):\n",
    "                    data.append({\n",
    "                        \"file\": os.path.join(root, file),\n",
    "                        \"hdr\": hdr_file,\n",
    "                        \"mango_id\": mango_id,\n",
    "                        \"day\": day\n",
    "                    })\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "train_ids, test_ids = train_test_split(data[\"mango_id\"].unique(), test_size=0.2, random_state=42)\n",
    "val_ids, test_ids = train_test_split(test_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = data[data[\"mango_id\"].isin(train_ids)]\n",
    "val_data = data[data[\"mango_id\"].isin(val_ids)]\n",
    "test_data = data[data[\"mango_id\"].isin(test_ids)]\n",
    "\n",
    "train_pairs, train_labels = create_pairs(train_data)\n",
    "val_pairs, val_labels = create_pairs(val_data)\n",
    "test_pairs, test_labels = create_pairs(test_data)\n",
    "\n",
    "train_pairs, train_labels = validate_pairs_and_labels(train_pairs, train_labels)\n",
    "val_pairs, val_labels = validate_pairs_and_labels(val_pairs, val_labels)\n",
    "test_pairs, test_labels = validate_pairs_and_labels(test_pairs, test_labels)\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class MangoPairDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file1, file2 = self.pairs[idx]\n",
    "        hdr1 = file1.replace(\".bin\", \".hdr\")\n",
    "        hdr2 = file2.replace(\".bin\", \".hdr\")\n",
    "        img1 = read_hyperspectral_image(hdr1)\n",
    "        img2 = read_hyperspectral_image(hdr2)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return img1, img2, label\n",
    "\n",
    "train_dataset = MangoPairDataset(train_pairs, train_labels)\n",
    "val_dataset = MangoPairDataset(val_pairs, val_labels)\n",
    "test_dataset = MangoPairDataset(test_pairs, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Vision Transformer Model ---\n",
    "class ViTRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTRegression, self).__init__()\n",
    "        self.base_model = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        in_features = self.base_model.heads[0].in_features\n",
    "        self.base_model.heads = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        x1 = self.base_model(img1)\n",
    "        x2 = self.base_model(img2)\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        return diff\n",
    "\n",
    "model = ViTRegression().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "class EarlyStopping:\n",
    "    \"\"\"Stop training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=5, delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model, path='checkpoint.pth'):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), path)\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved, saving model to {path}.\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader, epochs, patience):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for img1, img2, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            img1, img2, labels = img1.to(DEVICE), img2.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                img1, img2, labels = img1.to(DEVICE), img2.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(img1, img2).squeeze()\n",
    "                outputs = torch.round(outputs)  # Round predictions\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping\n",
    "train_model_with_early_stopping(model, train_loader, val_loader, EPOCHS, patience=5)\n",
    "\n",
    "# --- Testing ---\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in test_loader:\n",
    "            img1, img2, labels = img1.to(DEVICE), img2.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            outputs = torch.round(outputs)  # Round predictions\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_regression_metrics(model, data_loader):\n",
    "    \"\"\"Evaluate regression metrics for a given data loader.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in tqdm(data_loader, desc=\"Evaluating Metrics\"):\n",
    "            img1, img2, labels = img1.to(DEVICE), img2.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    print(all_labels)\n",
    "    print(all_predictions)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(all_labels, all_predictions)\n",
    "    mse = mean_squared_error(all_labels, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_labels, all_predictions)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    return mae, mse, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Metrics: 100%|██████████| 5/5 [00:02<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 6. 7. 1. 2. 3. 5. 6. 1. 2. 4. 5. 1. 3. 4. 2. 3. 1. 1. 2. 3.\n",
      " 4. 6. 7. 1. 2. 3. 5. 6. 1. 2. 4. 5. 1. 3. 4. 2. 3. 1. 1. 2. 3. 4. 6. 7.\n",
      " 8. 9. 1. 2. 3. 5. 6. 7. 8. 1. 2. 4. 5. 6. 7. 1. 3. 4. 5. 6. 2. 3. 4. 5.\n",
      " 1. 2. 3. 1. 2. 1.]\n",
      "[1.3223634  2.5293396  0.6836697  0.2848673  0.27115262 2.1025107\n",
      " 1.2069762  2.006033   1.0374961  1.593516   3.424874   3.2130094\n",
      " 2.2444723  2.8004923  4.6318502  0.968537   0.41251707 1.418841\n",
      " 0.5560199  2.387378   1.8313581  0.15758896 0.43706942 0.19366884\n",
      " 0.39808083 0.29616737 0.37182617 0.27948046 0.03607988 0.24049187\n",
      " 0.13857841 0.21423721 0.24340057 0.03898859 0.14090204 0.06524324\n",
      " 0.20441198 0.10249853 0.17815733 0.10191345 0.02625465 0.0756588\n",
      " 0.5825444  0.8139288  3.3833828  4.183477   4.012176   1.7518167\n",
      " 3.2988894  4.7790837  0.2313844  3.9659271  4.7660217  4.594721\n",
      " 2.334361   3.8814337  5.3616285  4.1973114  4.997406   4.826105\n",
      " 2.5657454  4.1128182  5.593013   0.80009437 0.6287935  1.631566\n",
      " 0.0844934  1.3957012  0.17130089 2.4316604  0.88458776 0.5956068\n",
      " 2.2603595  0.7132869  0.7669077  1.5470726  3.0272672  1.4801946 ]\n",
      "Mean Absolute Error (MAE): 2.2623\n",
      "Mean Squared Error (MSE): 7.8629\n",
      "Root Mean Squared Error (RMSE): 2.8041\n",
      "R^2 Score: -0.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.262258, 7.8629026, 2.8040867, -0.7855292416547242)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_regression_metrics(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
